{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup using the LOGO-CV Approach - With Feature Selection and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV, SequentialFeatureSelector, SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data\n",
    "df = pd.read_csv(\"processed_data_berlin_08042025.csv\")\n",
    "\n",
    "# And, drop stations with ≤ 2 years of data\n",
    "station_years = df.groupby('station_name')['year'].nunique()\n",
    "valid_stations = station_years[station_years > 2].index\n",
    "df_filtered = df[df['station_name'].isin(valid_stations)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Let's set up LOGO-CV\n",
    "logo = LeaveOneGroupOut()\n",
    "groups = df_filtered['station_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all predictions\n",
    "all_actuals, all_predictions = [], []\n",
    "aadb_actual_list, aadb_pred_list, aadb_station_list = [], [], []\n",
    "\n",
    "# Run LOGO-CV\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "    \n",
    "    # Compute mean log volume per station (baseline)\n",
    "    mean_log_volume = train_df['log_cycling_volume'].mean()\n",
    "    pred_log = np.full(len(test_df), mean_log_volume)\n",
    "    pred = np.expm1(pred_log)\n",
    "\n",
    "    y_true = test_df['cycling_volume'].values\n",
    "\n",
    "    # Daily predictions\n",
    "    all_actuals.extend(y_true)\n",
    "    all_predictions.extend(pred)\n",
    "\n",
    "    # AADB-level for this test station\n",
    "    aadb_actual = y_true.mean()\n",
    "    aadb_pred = pred.mean()\n",
    "    aadb_actual_list.append(aadb_actual)\n",
    "    aadb_pred_list.append(aadb_pred)\n",
    "    aadb_station_list.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to arrays\n",
    "y_true_all = np.array(all_actuals)\n",
    "y_pred_all = np.array(all_predictions)\n",
    "\n",
    "# Daily metrics\n",
    "mae_daily = mean_absolute_error(y_true_all, y_pred_all)\n",
    "rmse_daily = np.sqrt(mean_squared_error(y_true_all, y_pred_all))\n",
    "smape_daily = smape(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB-level metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_station_list,\n",
    "    'actual': aadb_actual_list,\n",
    "    'predicted': aadb_pred_list\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Baseline Model Evaluation:\n",
      "Daily  → MAE: 2193.79, RMSE: 3252.34, SMAPE: 72.64%\n",
      "AADB   → MAE: 1974.91, RMSE: 2573.91, SMAPE: 65.24%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nLOGO Baseline Model Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define full feature list\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "\n",
    "X = df_filtered[features]\n",
    "y = df_filtered['log_cycling_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by RFE for Linear Regression:\n",
      "['private_gardening', 'city_total_motor_vehicles', 'city_total_cars']\n"
     ]
    }
   ],
   "source": [
    "# RFE with Linear Regression\n",
    "lr = LinearRegression()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('feature_selection', RFECV(\n",
    "        estimator=lr,\n",
    "        step=1,\n",
    "        cv=KFold(n_splits=5),\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('regression', lr)\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Extract selected features\n",
    "selected_features = X.columns[pipeline.named_steps['feature_selection'].support_]\n",
    "print(\"\\nSelected features by RFE for Linear Regression:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LOGO grouping\n",
    "groups = df_filtered['station_name']\n",
    "\n",
    "# Prep for storing results\n",
    "all_true, all_preds = [], []\n",
    "aadb_actuals, aadb_preds, aadb_stations = [], [], []\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_cycling_volume']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['cycling_volume']\n",
    "\n",
    "    # Scale inputs\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train and predict\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_log = model.predict(X_test_scaled)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Evaluation\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Evaluation\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Linear Regression Evaluation:\n",
      "Daily  → MAE: 1893.86, RMSE: 2899.07, SMAPE: 62.04%\n",
      "AADB   → MAE: 1635.84, RMSE: 2269.95, SMAPE: 51.67%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Linear Regression Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full feature set\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "\n",
    "X = df_filtered[features]\n",
    "y = df_filtered['log_cycling_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by SFS for Decision Tree:\n",
      "['longitude', 'shops_within_5km', 'education_within_2km', 'farming', 'forests', 'horticulture', 'waterways', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'wpgt', 'pres', 'tsun', 'wdir', 'public_holiday', 'avg_truck_speed', 'total_motor_trucks', 'city_avg_motor_speed', 'city_avg_car_speed', 'city_avg_truck_speed', 'city_total_trucks', 'day_of_week', 'is_weekend', 'month', 'bicycle_lane_type_separate', 'bicycle_lane_type_sidepath', 'bicycle_lane_type_track']\n"
     ]
    }
   ],
   "source": [
    "# Base model for feature selection\n",
    "base_tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Sequential forward selection\n",
    "sfs = SequentialFeatureSelector(\n",
    "    base_tree,\n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "print(\"\\nSelected features by SFS for Decision Tree:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "\n",
      "Best hyperparameters:\n",
      "{'criterion': 'friedman_mse', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features from SFS\n",
    "X_selected = df_filtered[selected_features]\n",
    "y = df_filtered['log_cycling_volume']\n",
    "\n",
    "# Parameter grid for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    tree,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Save best model for reuse\n",
    "best_tree_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Setup LOGO\n",
    "groups = df_filtered['station_name']\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over stations\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_cycling_volume']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['cycling_volume']\n",
    "\n",
    "    # Train with best hyperparameters\n",
    "    tree_model = DecisionTreeRegressor(**grid_search.best_params_, random_state=42)\n",
    "    tree_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_log = tree_model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Evaluation\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Evaluation\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Decision Tree Evaluation:\n",
      "Daily  → MAE: 1908.66, RMSE: 2864.92, SMAPE: 53.04%\n",
      "AADB   → MAE: 1480.34, RMSE: 1880.08, SMAPE: 42.02%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nLOGO Decision Tree Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full feature list\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "\n",
    "X = df_filtered[features]\n",
    "y = df_filtered['log_cycling_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by RFE for Random Forest:\n",
      "['latitude', 'longitude', 'maxspeed_near_station', 'shops_within_0km', 'shops_within_1km', 'shops_within_2km', 'shops_within_5km', 'hotels_within_0km', 'hotels_within_1km', 'hotels_within_2km', 'hotels_within_5km', 'education_within_0km', 'education_within_1km', 'education_within_2km', 'education_within_5km', 'hospitals_within_0km', 'hospitals_within_1km', 'hospitals_within_2km', 'hospitals_within_5km', 'cemeteries', 'farming', 'forests', 'horticulture', 'industry', 'parks', 'private_gardening', 'residential', 'traffic', 'waterways', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'wpgt', 'pres', 'tsun', 'wdir', 'public_holiday', 'avg_motor_volume', 'avg_motor_speed', 'avg_car_speed', 'avg_truck_speed', 'total_motor_cars', 'total_motor_trucks', 'city_avg_motor_speed', 'city_avg_car_speed', 'city_avg_truck_speed', 'city_total_motor_vehicles', 'city_total_cars', 'city_total_trucks', 'day_of_week', 'is_weekend', 'month', 'bicycle_lane_type_none', 'bicycle_lane_type_separate', 'bicycle_lane_type_sidepath', 'bicycle_lane_type_track', 'bicycle_lane_type_unknown', 'distance_to_center_km']\n"
     ]
    }
   ],
   "source": [
    "# RFECV with Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=rf_model,\n",
    "    step=1,\n",
    "    cv=KFold(n_splits=5),\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X.columns[rfecv.support_]\n",
    "print(\"\\nSelected features by RFE for Random Forest:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Best hyperparameters for Random Forest:\n",
      "{'bootstrap': True, 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_selected = df_filtered[selected_features]\n",
    "y = df_filtered['log_cycling_volume']\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Random Forest:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store best model\n",
    "best_rf_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = df_filtered['station_name']\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGO loop\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_cycling_volume']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['cycling_volume']\n",
    "\n",
    "    model = RandomForestRegressor(**grid_search.best_params_, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Random Forest Evaluation:\n",
      "Daily  → MAE: 1955.11, RMSE: 2857.10, SMAPE: 54.97%\n",
      "AADB   → MAE: 1747.95, RMSE: 2277.25, SMAPE: 48.37%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Random Forest Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full feature list\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "\n",
    "X = df_filtered[features]\n",
    "y = df_filtered['log_cycling_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 61 features.\n",
      "Fitting estimator with 60 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 57 features.\n",
      "Fitting estimator with 56 features.\n",
      "Fitting estimator with 55 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 52 features.\n",
      "Fitting estimator with 51 features.\n",
      "Fitting estimator with 50 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "\n",
      "Selected features by RFE for Gradient Boosting:\n",
      "['longitude', 'shops_within_0km', 'shops_within_2km', 'hotels_within_0km', 'hotels_within_1km', 'hotels_within_2km', 'education_within_0km', 'education_within_5km', 'industry', 'residential', 'tavg', 'tmax', 'prcp', 'snow', 'tsun', 'avg_motor_volume', 'avg_motor_speed', 'avg_truck_speed', 'city_avg_truck_speed', 'city_total_trucks', 'day_of_week', 'month', 'distance_to_center_km']\n"
     ]
    }
   ],
   "source": [
    "# RFECV with Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=gb_model,\n",
    "    step=1,\n",
    "    cv=KFold(n_splits=5),\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "selected_features = X.columns[rfecv.support_]\n",
    "print(\"\\nSelected features by RFE for Gradient Boosting:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "Best hyperparameters for Gradient Boosting:\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_selected = df_filtered[selected_features]\n",
    "y = df_filtered['log_cycling_volume']\n",
    "\n",
    "# Parameter grid for Gradient Boosting\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Gradient Boosting:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Save best model\n",
    "best_gb_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = df_filtered['station_name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_cycling_volume']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['cycling_volume']\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(**grid_search.best_params_, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    y_pred_log = gb_model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Gradient Boosting Evaluation:\n",
      "Daily  → MAE: 1496.72, RMSE: 2294.93, SMAPE: 49.86%\n",
      "AADB   → MAE: 1294.61, RMSE: 1796.31, SMAPE: 43.21%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Gradient Boosting Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define full feature list\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "\n",
    "X = df_filtered[features]\n",
    "y = df_filtered['log_cycling_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by SFS for XGBoost:\n",
      "['shops_within_0km', 'hotels_within_0km', 'education_within_2km', 'hospitals_within_0km', 'farming', 'horticulture', 'industry', 'parks', 'tavg', 'tmin', 'prcp', 'snow', 'wspd', 'wpgt', 'pres', 'tsun', 'wdir', 'avg_motor_volume', 'avg_truck_speed', 'total_motor_cars', 'total_motor_trucks', 'city_avg_truck_speed', 'city_total_motor_vehicles', 'city_total_cars', 'day_of_week', 'is_weekend', 'month', 'bicycle_lane_type_separate', 'bicycle_lane_type_sidepath', 'bicycle_lane_type_track']\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Sequential Forward Selection\n",
    "sfs = SequentialFeatureSelector(\n",
    "    xgb_model,\n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "print(\"\\nSelected features by SFS for XGBoost:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "\n",
      "Best hyperparameters for XGBoost:\n",
      "{'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_selected = df_filtered[selected_features]\n",
    "y = df_filtered['log_cycling_volume']\n",
    "\n",
    "# Parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [5, 10],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_weight': [1, 5],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for XGBoost:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store best model\n",
    "best_xgb_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = df_filtered['station_name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_cycling_volume']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['cycling_volume']\n",
    "\n",
    "    xgb_model = XGBRegressor(**grid_search.best_params_, objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_log = xgb_model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO XGBoost Evaluation:\n",
      "Daily  → MAE: 1366.49, RMSE: 2048.08, SMAPE: 45.01%\n",
      "AADB   → MAE: 1152.88, RMSE: 1465.96, SMAPE: 38.69%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO XGBoost Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define full feature list\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "\n",
    "X = df_filtered[features]\n",
    "y = df_filtered['log_cycling_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by SelectKBest for Shallow Neural Network:\n",
      "['latitude', 'maxspeed_near_station', 'shops_within_0km', 'shops_within_1km', 'shops_within_2km', 'shops_within_5km', 'hotels_within_1km', 'hotels_within_2km', 'hotels_within_5km', 'education_within_0km', 'education_within_1km', 'education_within_2km', 'education_within_5km', 'forests', 'parks', 'private_gardening', 'tavg', 'tmin', 'tmax', 'tsun', 'avg_motor_volume', 'avg_motor_speed', 'avg_car_speed', 'total_motor_cars', 'total_motor_trucks', 'city_avg_motor_speed', 'city_avg_truck_speed', 'city_total_trucks', 'day_of_week', 'is_weekend', 'distance_to_center_km']\n"
     ]
    }
   ],
   "source": [
    "# Pipeline: scale → SelectKBest\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select_k_best', SelectKBest(score_func=f_regression, k=31))  # You can adjust k here\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Get selected features\n",
    "mask = pipeline.named_steps['select_k_best'].get_support()\n",
    "selected_features = X.columns[mask]\n",
    "\n",
    "print(\"\\nSelected features by SelectKBest for Shallow Neural Network:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Best hyperparameters for Shallow Neural Network:\n",
      "{'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Use the selected features\n",
    "X_selected = df_filtered[selected_features]\n",
    "y = df_filtered['log_cycling_volume']\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Parameter grid for SNN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor(max_iter=500, random_state=42)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    mlp,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Shallow Neural Network:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store config\n",
    "best_snn_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Groups\n",
    "groups = df_filtered['station_name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_cycling_volume']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['cycling_volume']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    mlp_model = MLPRegressor(**best_snn_params, max_iter=500, random_state=42)\n",
    "    mlp_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_log = mlp_model.predict(X_test_scaled)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Shallow Neural Network Evaluation:\n",
      "Daily  → MAE: 11670.52, RMSE: 39299.67, SMAPE: 112.44%\n",
      "AADB   → MAE: 11562.97, RMSE: 24072.61, SMAPE: 106.15%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Shallow Neural Network Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "\n",
    "X = df_filtered[features]\n",
    "y = df_filtered['log_cycling_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Best hyperparameters for Deep Neural Network:\n",
      "{'mlp__activation': 'tanh', 'mlp__hidden_layer_sizes': (100, 100), 'mlp__learning_rate_init': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline for scaling + PCA + MLP\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=20)),\n",
    "    ('mlp', MLPRegressor(max_iter=500, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid for SNN\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(50, 50), (100, 50), (100, 100)],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Deep Neural Network:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store best model\n",
    "best_dnn_pipeline = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Groups\n",
    "groups = df_filtered['station_name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx]\n",
    "    test_df = df_filtered.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df['log_cycling_volume']\n",
    "    X_test = test_df[features]\n",
    "    y_test_true = test_df['cycling_volume']\n",
    "\n",
    "    # Rebuild same pipeline with best params\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=20)),\n",
    "        ('mlp', MLPRegressor(\n",
    "            hidden_layer_sizes=grid_search.best_params_['mlp__hidden_layer_sizes'],\n",
    "            activation=grid_search.best_params_['mlp__activation'],\n",
    "            learning_rate_init=grid_search.best_params_['mlp__learning_rate_init'],\n",
    "            max_iter=500,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_log = pipeline.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Deep Neural Network Evaluation:\n",
      "Daily  → MAE: 2520.83, RMSE: 3598.29, SMAPE: 83.98%\n",
      "AADB   → MAE: 2342.05, RMSE: 2952.58, SMAPE: 76.10%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Deep Neural Network Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "n_steps = 30\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "lstm_units = 64\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "non_features = ['station_name', 'date', 'year', 'cycling_volume', 'log_cycling_volume', 'num_motor_sources']\n",
    "features = [col for col in df_filtered.columns if col not in non_features]\n",
    "groups = df_filtered['station_name']\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_actuals = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train_idx, test_idx in logo.split(df_filtered, groups=groups):\n",
    "    train_df = df_filtered.iloc[train_idx].sort_values('date')\n",
    "    test_df = df_filtered.iloc[test_idx].sort_values('date')\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_df[features])\n",
    "    X_test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "    y_train = train_df['log_cycling_volume'].values\n",
    "    y_test_true = test_df['cycling_volume'].values\n",
    "    y_test_log = test_df['log_cycling_volume'].values\n",
    "\n",
    "    # Build sequences\n",
    "    def create_sequences(X, y, n_steps):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(n_steps, len(X)):\n",
    "            X_seq.append(X[i - n_steps:i])\n",
    "            y_seq.append(y[i])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, n_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_log, n_steps)\n",
    "    y_test_true_seq = y_test_true[n_steps:]\n",
    "\n",
    "    if len(X_test_seq) == 0:\n",
    "        continue\n",
    "\n",
    "    # Build model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, input_shape=(n_steps, len(features))))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "    model.fit(X_train_seq, y_train_seq, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_log = model.predict(X_test_seq).flatten()\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_actuals.extend(y_test_true_seq)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true_seq))\n",
    "    aadb_stations.append(test_df['station_name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_actuals, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_actuals, all_preds))\n",
    "r2_daily = r2_score(all_actuals, all_preds)\n",
    "smape_daily = smape(np.array(all_actuals), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO LSTM Evaluation:\n",
      "Daily  → MAE: 2927.27, RMSE: 4131.95, R²: -0.96, SMAPE: 146.65%\n",
      "AADB   → MAE: 2712.44, RMSE: 3509.52, R²: -1.61, SMAPE: 131.54%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO LSTM Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
