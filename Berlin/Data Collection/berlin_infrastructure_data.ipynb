{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdd6351-733c-4e53-8ade-13c1f3d314df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import osmium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49eedf-6c27-4155-b67f-443e267485ff",
   "metadata": {},
   "source": [
    "## Distance to the City Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d574076-a7d1-43e2-8995-83e77edcc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the cycling data\n",
    "df = pd.read_csv(\"cycling_data_berlin_07032025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0379d90-7fd7-4805-a77b-41a36f754a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the city center coordinates - Alexanderplatz\n",
    "city_center = (52.5220, 13.4133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb920186-4d1f-49c9-9f6b-b49c44e7d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's take out the unique stations\n",
    "unique_stations = df[['station_name', 'latitude', 'longitude']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70dc11e3-12f8-44ff-9ae3-f7b394d49c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's define a function to calculate distance\n",
    "def calculate_distance_to_center(lat, lon, center=city_center):\n",
    "    return geodesic((lat, lon), center).kilometers\n",
    "\n",
    "# And, apply the function to each station\n",
    "unique_stations['distance_to_center'] = unique_stations.apply(\n",
    "    lambda row: calculate_distance_to_center(row['latitude'], row['longitude']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05809491-d91c-406e-a178-31028d4cbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, now merge back to the original dataset\n",
    "df = df.merge(unique_stations, on=['station_name', 'latitude', 'longitude'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae2aacc-deb1-4500-bc4a-45b58c05e867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1248009 entries, 0 to 1248008\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   timestamp           1248009 non-null  object \n",
      " 1   station_name        1248009 non-null  object \n",
      " 2   latitude            1248009 non-null  float64\n",
      " 3   longitude           1248009 non-null  float64\n",
      " 4   cycling_volume      1248009 non-null  float64\n",
      " 5   distance_to_center  1248009 non-null  float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 57.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Let's check how the dataset looks now\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18764c5a-e258-41a6-83a7-9fcbf77d7ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cycling_volume</th>\n",
       "      <th>distance_to_center</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.137726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.137726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.137726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.137726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.137726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp    station_name   latitude  longitude  cycling_volume  \\\n",
       "0  2015-01-01 00:00:00  Schwedter Steg  52.549072  13.400367             8.0   \n",
       "1  2015-01-01 01:00:00  Schwedter Steg  52.549072  13.400367            10.0   \n",
       "2  2015-01-01 02:00:00  Schwedter Steg  52.549072  13.400367             8.0   \n",
       "3  2015-01-01 03:00:00  Schwedter Steg  52.549072  13.400367             6.0   \n",
       "4  2015-01-01 04:00:00  Schwedter Steg  52.549072  13.400367             6.0   \n",
       "\n",
       "   distance_to_center  \n",
       "0            3.137726  \n",
       "1            3.137726  \n",
       "2            3.137726  \n",
       "3            3.137726  \n",
       "4            3.137726  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce95cb55-4b9e-432f-9b61-a9f86713d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, save the updated dataset\n",
    "df.to_csv(\"cycling_data_berlin_08032025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ec4c2-616f-492d-802d-db366c4dacb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Unique Stations and Unique Year for the Bicycle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff326c8e-e24d-481d-995f-d1bca3859d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the bicycle data\n",
    "data = pd.read_csv(\"cycling_data_berlin_08032025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff12c0bc-448e-401b-8a92-d9b9123b919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And make sure that the timestamp is in the required format\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Now, let's add the year column to the dataset\n",
    "data['year'] = data['timestamp'].dt.year\n",
    "\n",
    "# Extract unique station-year combinations\n",
    "unique_station_year = data[['station_name', 'latitude', 'longitude', 'year']].drop_duplicates()\n",
    "\n",
    "# Save the sheet for future use\n",
    "unique_station_year.to_csv(\"unique_station_year_berlin.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0214cfa-29a1-4d85-9aee-63233ca5267b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Max Speed Near the Counting Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c31f69-9c12-482f-8151-63db197a9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the folder containing OSM files\n",
    "osm_folder = \"berlin_osm_datasets\"\n",
    "\n",
    "# And, load the station data\n",
    "station_df = pd.read_csv(\"unique_station_year_berlin.csv\")\n",
    "\n",
    "# And, ensure the year column is treated as an integer\n",
    "station_df['year'] = station_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342ed0f2-055e-4eb2-aee7-9ca7d3e8b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each year to its corresponding OSM file\n",
    "year_to_osm = {year: f\"berlin-{str(year)[-2:]}0101.osm.pbf\" for year in range(2015, 2024)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b872aa-79a1-48ab-b4f4-a8ca8d0ecf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's define the maxspeed search radius (in meters)\n",
    "MAXSPEED_RADIUS = 100  \n",
    "\n",
    "# And also define the haversine function to calculate distances between two lat/lon points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance in meters between two lat/lon points.\"\"\"\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "869fdb17-7b6f-4038-8d76-eae1def8aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also define a class to extract roads with maxspeed values from OSM\n",
    "class MaxSpeedHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.roads = []\n",
    "\n",
    "    def way(self, w):\n",
    "        if 'maxspeed' in w.tags and w.nodes:\n",
    "            try:\n",
    "                maxspeed = int(w.tags['maxspeed'].split()[0])  # Extract numeric value\n",
    "            except ValueError:\n",
    "                return  # Skip invalid values\n",
    "            \n",
    "            # Ensure node location is valid before accessing\n",
    "            if w.nodes[0].location.valid():\n",
    "                self.roads.append({\n",
    "                    'lat': w.nodes[0].location.lat,\n",
    "                    'lon': w.nodes[0].location.lon,\n",
    "                    'maxspeed': maxspeed\n",
    "                })\n",
    "\n",
    "def extract_maxspeed(osm_file):\n",
    "    \"\"\"Extract road segments with maxspeed information from a given OSM file.\"\"\"\n",
    "    handler = MaxSpeedHandler()\n",
    "\n",
    "    # Enable location lookup\n",
    "    handler.apply_file(osm_file, locations=True)  \n",
    "\n",
    "    return pd.DataFrame(handler.roads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d6679a-7eed-457a-a7c6-342bf98721e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing maxspeed data for year 2015 using berlin_osm_datasets\\berlin-150101.osm.pbf...\n",
      "Processing maxspeed data for year 2016 using berlin_osm_datasets\\berlin-160101.osm.pbf...\n",
      "Processing maxspeed data for year 2017 using berlin_osm_datasets\\berlin-170101.osm.pbf...\n",
      "Processing maxspeed data for year 2018 using berlin_osm_datasets\\berlin-180101.osm.pbf...\n",
      "Processing maxspeed data for year 2019 using berlin_osm_datasets\\berlin-190101.osm.pbf...\n",
      "Processing maxspeed data for year 2020 using berlin_osm_datasets\\berlin-200101.osm.pbf...\n",
      "Processing maxspeed data for year 2021 using berlin_osm_datasets\\berlin-210101.osm.pbf...\n",
      "Processing maxspeed data for year 2022 using berlin_osm_datasets\\berlin-220101.osm.pbf...\n",
      "Processing maxspeed data for year 2023 using berlin_osm_datasets\\berlin-230101.osm.pbf...\n"
     ]
    }
   ],
   "source": [
    "# List to store maxspeed results\n",
    "maxspeed_results = []\n",
    "\n",
    "# Iterate over unique years in the station data\n",
    "for year in station_df['year'].unique():\n",
    "    osm_file_path = os.path.join(osm_folder, year_to_osm[year])\n",
    "    \n",
    "    if not os.path.exists(osm_file_path):\n",
    "        print(f\"Warning: OSM file for year {year} not found: {osm_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing maxspeed data for year {year} using {osm_file_path}...\")\n",
    "\n",
    "    # Extract maxspeed information\n",
    "    maxspeed_df = extract_maxspeed(osm_file_path)\n",
    "\n",
    "    # Iterate over stations for this year\n",
    "    for index, station in station_df[station_df['year'] == year].iterrows():\n",
    "        lat_s, lon_s, station_name = station['latitude'], station['longitude'], station['station_name']\n",
    "\n",
    "        # Compute distances to all road segments with maxspeed\n",
    "        distances = maxspeed_df.apply(lambda road: haversine(lat_s, lon_s, road['lat'], road['lon']), axis=1)\n",
    "\n",
    "        # Get maxspeed values within 500m\n",
    "        valid_maxspeeds = maxspeed_df.loc[distances <= MAXSPEED_RADIUS, 'maxspeed']\n",
    "\n",
    "        # Get the maximum maxspeed found near the station (or NaN if none found)\n",
    "        max_maxspeed = valid_maxspeeds.max() if not valid_maxspeeds.empty else None\n",
    "\n",
    "        # Store results\n",
    "        maxspeed_results.append({\n",
    "            'station_name': station_name,\n",
    "            'year': year,\n",
    "            'maxspeed_near_station': max_maxspeed\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f28a199-4644-40fb-ae64-3c302ac84ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          station_name   latitude  longitude  year  maxspeed_near_station\n",
      "0       Schwedter Steg  52.549072  13.400367  2015                   70.0\n",
      "1      Jannowitzbrücke  52.513936  13.417722  2015                   80.0\n",
      "2  Prinzregentenstraße  52.488136  13.333120  2015                   50.0\n",
      "3          Yorckstraße  52.492110  13.373341  2015                  120.0\n",
      "4           Markstraße  52.558190  13.364944  2015                   50.0\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "maxspeed_df = pd.DataFrame(maxspeed_results)\n",
    "\n",
    "# Merge maxspeed info into the original station dataset\n",
    "station_df = station_df.merge(maxspeed_df, on=['station_name', 'year'], how='left')\n",
    "\n",
    "# Save updated dataset\n",
    "station_df.to_csv(\"stations_with_maxspeed.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(station_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0f9d6-8dc0-4864-8fac-a576ba2495dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bicycle Lane Type Near the Counting Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3c29b3-cb2a-4e2a-af21-3f9308869acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the folder containing OSM files\n",
    "osm_folder = \"berlin_osm_datasets\"\n",
    "\n",
    "# And, load the station data\n",
    "station_df = pd.read_csv(\"unique_station_year_berlin.csv\")\n",
    "\n",
    "# And, ensure the year column is treated as an integer\n",
    "station_df['year'] = station_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c7860a-f156-4ff3-874e-e21e58a1df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each year to its corresponding OSM file\n",
    "year_to_osm = {year: f\"berlin-{str(year)[-2:]}0101.osm.pbf\" for year in range(2015, 2024)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82685ac7-c30e-4a45-9cf4-3b4bae54d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the search radius (in meters)\n",
    "CYCLEWAY_RADIUS = 100 \n",
    "\n",
    "# Let's define the haversine function to calculate distances between two lat/lon points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance in meters between two lat/lon points.\"\"\"\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec18d3a6-11da-4ad2-b6bb-8317d745d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also define a class to extract bicycle lane type with maxspeed values from OSM\n",
    "class CyclewayHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cycleways = []\n",
    "\n",
    "    def way(self, w):\n",
    "        # Extract cycleway-related ways\n",
    "        if any(tag in w.tags for tag in ['cycleway', 'cycleway:left', 'cycleway:right', 'cycleway:both']):\n",
    "            # Ensure node has valid location before accessing\n",
    "            if w.nodes[0].location.valid():\n",
    "                self.cycleways.append({\n",
    "                    'lat': w.nodes[0].location.lat,\n",
    "                    'lon': w.nodes[0].location.lon,\n",
    "                    'bicycle_lane_type': w.tags.get('cycleway', 'unknown')  # Default to 'unknown' if missing\n",
    "                })\n",
    "\n",
    "def extract_cycleways(osm_file):\n",
    "    \"\"\"Extract cycleway-related data from a given OSM file with location resolution.\"\"\"\n",
    "    handler = CyclewayHandler()\n",
    "    \n",
    "    # Enable location lookup to resolve node positions\n",
    "    handler.apply_file(osm_file, locations=True)\n",
    "\n",
    "    return pd.DataFrame(handler.cycleways)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0daa8c5-2d75-4ba1-9aaf-8f572ad7db41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bicycle lane data for year 2015 using berlin_osm_datasets\\berlin-150101.osm.pbf...\n",
      "Processing bicycle lane data for year 2016 using berlin_osm_datasets\\berlin-160101.osm.pbf...\n",
      "Processing bicycle lane data for year 2017 using berlin_osm_datasets\\berlin-170101.osm.pbf...\n",
      "Processing bicycle lane data for year 2018 using berlin_osm_datasets\\berlin-180101.osm.pbf...\n",
      "Processing bicycle lane data for year 2019 using berlin_osm_datasets\\berlin-190101.osm.pbf...\n",
      "Processing bicycle lane data for year 2020 using berlin_osm_datasets\\berlin-200101.osm.pbf...\n",
      "Processing bicycle lane data for year 2021 using berlin_osm_datasets\\berlin-210101.osm.pbf...\n",
      "Processing bicycle lane data for year 2022 using berlin_osm_datasets\\berlin-220101.osm.pbf...\n",
      "Processing bicycle lane data for year 2023 using berlin_osm_datasets\\berlin-230101.osm.pbf...\n"
     ]
    }
   ],
   "source": [
    "# List to store cycleway results\n",
    "cycleway_results = []\n",
    "\n",
    "# Iterate over unique years in the station data\n",
    "for year in station_df['year'].unique():\n",
    "    osm_file_path = os.path.join(osm_folder, year_to_osm[year])\n",
    "    \n",
    "    if not os.path.exists(osm_file_path):\n",
    "        print(f\"Warning: OSM file for year {year} not found: {osm_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing bicycle lane data for year {year} using {osm_file_path}...\")\n",
    "\n",
    "    # Extract cycleway information\n",
    "    cycleway_df = extract_cycleways(osm_file_path)\n",
    "\n",
    "    # Iterate over stations for this year\n",
    "    for index, station in station_df[station_df['year'] == year].iterrows():\n",
    "        lat_s, lon_s, station_name = station['latitude'], station['longitude'], station['station_name']\n",
    "\n",
    "        # Compute distances to all cycleways\n",
    "        distances = cycleway_df.apply(lambda way: haversine(lat_s, lon_s, way['lat'], way['lon']), axis=1)\n",
    "\n",
    "        # Get cycleway types within 500m\n",
    "        valid_cycleways = cycleway_df.loc[distances <= CYCLEWAY_RADIUS, 'bicycle_lane_type']\n",
    "\n",
    "        # Select the most common bicycle lane type near the station\n",
    "        most_common_lane = valid_cycleways.mode()[0] if not valid_cycleways.empty else \"none\"\n",
    "\n",
    "        # Store results\n",
    "        cycleway_results.append({\n",
    "            'station_name': station_name,\n",
    "            'year': year,\n",
    "            'bicycle_lane_type': most_common_lane\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e823d97-6fde-4b94-ad14-da8898a093e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          station_name   latitude  longitude  year bicycle_lane_type\n",
      "0       Schwedter Steg  52.549072  13.400367  2015              none\n",
      "1      Jannowitzbrücke  52.513936  13.417722  2015              lane\n",
      "2  Prinzregentenstraße  52.488136  13.333120  2015              lane\n",
      "3          Yorckstraße  52.492110  13.373341  2015              none\n",
      "4           Markstraße  52.558190  13.364944  2015             track\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "cycleway_df = pd.DataFrame(cycleway_results)\n",
    "\n",
    "# Merge bicycle lane info into the original station dataset\n",
    "station_df = station_df.merge(cycleway_df, on=['station_name', 'year'], how='left')\n",
    "\n",
    "# Save updated dataset\n",
    "station_df.to_csv(\"stations_with_bicycle_lane_type.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(station_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bba9cb-c756-4260-8241-4b042705bde4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Number of Shops within a radius of 0.5, 1, 2 & 5km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aeb6b18-22a0-4052-92c4-4b5aca995c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the folder containing OSM files\n",
    "osm_folder = \"berlin_osm_datasets\"\n",
    "\n",
    "# And, load the station data\n",
    "station_df = pd.read_csv(\"unique_station_year_berlin.csv\")\n",
    "\n",
    "# And, ensure the year column is treated as an integer\n",
    "station_df['year'] = station_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ad83ec2-dbe6-430e-80a4-95da6e362e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each year to its corresponding OSM file\n",
    "year_to_osm = {year: f\"berlin-{str(year)[-2:]}0101.osm.pbf\" for year in range(2015, 2024)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcbe8bc4-c425-4d3f-a71c-72ed02298628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the haversine function to calculate distances between two lat/lon points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance in meters between two lat/lon points.\"\"\"\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17b4c4e6-c93d-4e1f-bc09-c7613103858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also define a class to extract number of shops from OSM\n",
    "class ShopHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shops = []\n",
    "\n",
    "    def node(self, n):\n",
    "        if 'shop' in n.tags:\n",
    "            self.shops.append({'lat': n.location.lat, 'lon': n.location.lon})\n",
    "\n",
    "def extract_shops(osm_file):\n",
    "    \"\"\"Extract shop locations from a given OSM file.\"\"\"\n",
    "    handler = ShopHandler()\n",
    "    handler.apply_file(osm_file)\n",
    "    return pd.DataFrame(handler.shops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f35f5c7-9f11-4615-bb75-154a3218b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2015 using berlin_osm_datasets\\berlin-150101.osm.pbf...\n",
      "Processing year 2016 using berlin_osm_datasets\\berlin-160101.osm.pbf...\n",
      "Processing year 2017 using berlin_osm_datasets\\berlin-170101.osm.pbf...\n",
      "Processing year 2018 using berlin_osm_datasets\\berlin-180101.osm.pbf...\n",
      "Processing year 2019 using berlin_osm_datasets\\berlin-190101.osm.pbf...\n",
      "Processing year 2020 using berlin_osm_datasets\\berlin-200101.osm.pbf...\n",
      "Processing year 2021 using berlin_osm_datasets\\berlin-210101.osm.pbf...\n",
      "Processing year 2022 using berlin_osm_datasets\\berlin-220101.osm.pbf...\n",
      "Processing year 2023 using berlin_osm_datasets\\berlin-230101.osm.pbf...\n"
     ]
    }
   ],
   "source": [
    "# Define the search radii (in meters)\n",
    "radii = [500, 1000, 2000, 5000]\n",
    "\n",
    "# Create an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over unique years in the station data\n",
    "for year in station_df['year'].unique():\n",
    "    osm_file_path = os.path.join(osm_folder, year_to_osm[year])\n",
    "    \n",
    "    if not os.path.exists(osm_file_path):\n",
    "        print(f\"Warning: OSM file for year {year} not found: {osm_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing year {year} using {osm_file_path}...\")\n",
    "    \n",
    "    # Extract shop locations for the given year\n",
    "    shop_df = extract_shops(osm_file_path)\n",
    "    \n",
    "    # Iterate over stations for the given year\n",
    "    for index, station in station_df[station_df['year'] == year].iterrows():\n",
    "        lat_s, lon_s, station_name = station['latitude'], station['longitude'], station['station_name']\n",
    "        \n",
    "        # Compute distances to all shops\n",
    "        distances = shop_df.apply(lambda shop: haversine(lat_s, lon_s, shop['lat'], shop['lon']), axis=1)\n",
    "        \n",
    "        # Count shops within each radius\n",
    "        shop_counts = {f'shops_within_{r//1000}km': (distances <= r).sum() for r in radii}\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'station_name': station_name,\n",
    "            'year': year,\n",
    "            **shop_counts\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d875a076-05c6-497d-b75d-2f9d1680dcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          station_name  year  shops_within_0km  shops_within_1km  \\\n",
      "0       Schwedter Steg  2015                 6               162   \n",
      "1      Jannowitzbrücke  2015                31               100   \n",
      "2  Prinzregentenstraße  2015                43               187   \n",
      "3          Yorckstraße  2015                31               155   \n",
      "4           Markstraße  2015                 7                93   \n",
      "\n",
      "   shops_within_2km  shops_within_5km  \n",
      "0               743              2421  \n",
      "1               759              3729  \n",
      "2              1141              3386  \n",
      "3               676              4073  \n",
      "4               484              2091  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Merge bicycle lane info into the original station dataset\n",
    "station_df = station_df.merge(result_df, on=['station_name', 'year'], how='left')\n",
    "\n",
    "# Save updated dataset\n",
    "station_df.to_csv(\"stations_with_shops.csv\", index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3629f5-a73e-40be-b298-c35fad114edc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Number of Hotels within a radius of 0.5, 1, 2 & 5km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f678e2bb-2b8f-4d97-9a3b-74cd8aef8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the folder containing OSM files\n",
    "osm_folder = \"berlin_osm_datasets\"\n",
    "\n",
    "# And, load the station data\n",
    "station_df = pd.read_csv(\"unique_station_year_berlin.csv\")\n",
    "\n",
    "# And, ensure the year column is treated as an integer\n",
    "station_df['year'] = station_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1eab9c31-1246-4c91-b20b-4161abc6305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each year to its corresponding OSM file\n",
    "year_to_osm = {year: f\"berlin-{str(year)[-2:]}0101.osm.pbf\" for year in range(2015, 2024)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60d51358-49d1-41fa-8852-714aab1622d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the search radii in meters\n",
    "radii = [500, 1000, 2000, 5000]  # 0.5km, 1km, 2km, 5km\n",
    "\n",
    "# And, let's define the haversine function to calculate distances\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance in meters between two lat/lon points.\"\"\"\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "132b48d1-943c-475d-a3cc-281f1345be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's define a class to extract hotel locations from OSM\n",
    "class HotelHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hotels = []\n",
    "\n",
    "    def node(self, n):\n",
    "        if 'tourism' in n.tags and n.tags['tourism'] == 'hotel':\n",
    "            self.hotels.append({\n",
    "                'lat': n.location.lat,\n",
    "                'lon': n.location.lon\n",
    "            })\n",
    "\n",
    "def extract_hotels(osm_file):\n",
    "    \"\"\"Extract hotel locations from a given OSM file.\"\"\"\n",
    "    handler = HotelHandler()\n",
    "    \n",
    "    # Enable location lookup\n",
    "    handler.apply_file(osm_file, locations=True)\n",
    "\n",
    "    return pd.DataFrame(handler.hotels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4846655-6cf9-4f04-bc43-b4df3df72774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hotel data for year 2015 using berlin_osm_datasets\\berlin-150101.osm.pbf...\n",
      "Processing hotel data for year 2016 using berlin_osm_datasets\\berlin-160101.osm.pbf...\n",
      "Processing hotel data for year 2017 using berlin_osm_datasets\\berlin-170101.osm.pbf...\n",
      "Processing hotel data for year 2018 using berlin_osm_datasets\\berlin-180101.osm.pbf...\n",
      "Processing hotel data for year 2019 using berlin_osm_datasets\\berlin-190101.osm.pbf...\n",
      "Processing hotel data for year 2020 using berlin_osm_datasets\\berlin-200101.osm.pbf...\n",
      "Processing hotel data for year 2021 using berlin_osm_datasets\\berlin-210101.osm.pbf...\n",
      "Processing hotel data for year 2022 using berlin_osm_datasets\\berlin-220101.osm.pbf...\n",
      "Processing hotel data for year 2023 using berlin_osm_datasets\\berlin-230101.osm.pbf...\n"
     ]
    }
   ],
   "source": [
    "# List to store hotel count results\n",
    "hotel_results = []\n",
    "\n",
    "# Iterate over unique years\n",
    "for year in station_df['year'].unique():\n",
    "    osm_file_path = os.path.join(osm_folder, year_to_osm[year])\n",
    "    \n",
    "    if not os.path.exists(osm_file_path):\n",
    "        print(f\"Warning: OSM file for year {year} not found: {osm_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing hotel data for year {year} using {osm_file_path}...\")\n",
    "\n",
    "    # Extract hotel locations\n",
    "    hotel_df = extract_hotels(osm_file_path)\n",
    "\n",
    "    # Iterate over stations for this year\n",
    "    for index, station in station_df[station_df['year'] == year].iterrows():\n",
    "        lat_s, lon_s, station_name = station['latitude'], station['longitude'], station['station_name']\n",
    "\n",
    "        # Compute distances to all hotels\n",
    "        distances = hotel_df.apply(lambda hotel: haversine(lat_s, lon_s, hotel['lat'], hotel['lon']), axis=1)\n",
    "\n",
    "        # Count hotels within each radius\n",
    "        hotel_counts = {f'hotels_within_{r//1000}km': (distances <= r).sum() for r in radii}\n",
    "\n",
    "        # Store results\n",
    "        hotel_results.append({\n",
    "            'station_name': station_name,\n",
    "            'year': year,\n",
    "            **hotel_counts\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "342e96a8-4c25-4c30-91be-12b6d8e150bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          station_name   latitude  longitude  year  hotels_within_0km  \\\n",
      "0       Schwedter Steg  52.549072  13.400367  2015                  0   \n",
      "1      Jannowitzbrücke  52.513936  13.417722  2015                  3   \n",
      "2  Prinzregentenstraße  52.488136  13.333120  2015                  2   \n",
      "3          Yorckstraße  52.492110  13.373341  2015                  0   \n",
      "4           Markstraße  52.558190  13.364944  2015                  0   \n",
      "\n",
      "   hotels_within_1km  hotels_within_2km  hotels_within_5km  \n",
      "0                  4                 11                115  \n",
      "1                  8                 46                168  \n",
      "2                  4                 81                199  \n",
      "3                 10                 38                242  \n",
      "4                  0                  2                 77  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "hotel_df = pd.DataFrame(hotel_results)\n",
    "\n",
    "# Merge hotel count info into the original station dataset\n",
    "station_df = station_df.merge(hotel_df, on=['station_name', 'year'], how='left')\n",
    "\n",
    "# Save updated dataset\n",
    "station_df.to_csv(\"stations_with_hotels.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(station_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec830b61-a31c-44b7-8e3e-b97eee408e65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Number of Education within a radius of 0.5, 1, 2 & 5km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edda7f69-14eb-4419-8b6d-f334904e7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the folder containing OSM files\n",
    "osm_folder = \"berlin_osm_datasets\"\n",
    "\n",
    "# And, load the station data\n",
    "station_df = pd.read_csv(\"unique_station_year_berlin.csv\")\n",
    "\n",
    "# And, ensure the year column is treated as an integer\n",
    "station_df['year'] = station_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e75f6a74-edf3-432b-8a23-7106817e2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each year to its corresponding OSM file\n",
    "year_to_osm = {year: f\"berlin-{str(year)[-2:]}0101.osm.pbf\" for year in range(2015, 2024)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba8654c-0ae6-4cf7-8ef2-42931ac3da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the search radii in meters\n",
    "radii = [500, 1000, 2000, 5000]  # 0.5km, 1km, 2km, 5km\n",
    "\n",
    "# And, the haversine function to calculate distances\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance in meters between two lat/lon points.\"\"\"\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b24e81-2b85-497c-811f-0009c52d257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, also define a class to extract educational institutions from OSM\n",
    "class EducationHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.education_centers = []\n",
    "\n",
    "    def node(self, n):\n",
    "        if 'amenity' in n.tags and n.tags['amenity'] in [\n",
    "            'kindergarten', 'school', 'driving_school', 'college', 'university', 'music_school',\n",
    "            'childcare', 'research_institute', 'language_school', 'dancing_school', 'sailing_school',\n",
    "            'sport_school', 'boat_school', 'first_aid_school', 'art_school'\n",
    "        ]:\n",
    "            self.education_centers.append({\n",
    "                'lat': n.location.lat,\n",
    "                'lon': n.location.lon,\n",
    "                'type': n.tags['amenity']  # Store the type of educational institute\n",
    "            })\n",
    "\n",
    "def extract_educational_institutes(osm_file):\n",
    "    \"\"\"Extract educational institution locations from a given OSM file.\"\"\"\n",
    "    handler = EducationHandler()\n",
    "    \n",
    "    # Enable location lookup\n",
    "    handler.apply_file(osm_file, locations=True)\n",
    "\n",
    "    return pd.DataFrame(handler.education_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d700f6c-b4a9-4821-a638-577e7ff2839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing educational institute data for year 2015 using berlin_osm_datasets\\berlin-150101.osm.pbf...\n",
      "Processing educational institute data for year 2016 using berlin_osm_datasets\\berlin-160101.osm.pbf...\n",
      "Processing educational institute data for year 2017 using berlin_osm_datasets\\berlin-170101.osm.pbf...\n",
      "Processing educational institute data for year 2018 using berlin_osm_datasets\\berlin-180101.osm.pbf...\n",
      "Processing educational institute data for year 2019 using berlin_osm_datasets\\berlin-190101.osm.pbf...\n",
      "Processing educational institute data for year 2020 using berlin_osm_datasets\\berlin-200101.osm.pbf...\n",
      "Processing educational institute data for year 2021 using berlin_osm_datasets\\berlin-210101.osm.pbf...\n",
      "Processing educational institute data for year 2022 using berlin_osm_datasets\\berlin-220101.osm.pbf...\n",
      "Processing educational institute data for year 2023 using berlin_osm_datasets\\berlin-230101.osm.pbf...\n"
     ]
    }
   ],
   "source": [
    "# List to store educational institute count results\n",
    "education_results = []\n",
    "\n",
    "# Iterate over unique years\n",
    "for year in station_df['year'].unique():\n",
    "    osm_file_path = os.path.join(osm_folder, year_to_osm[year])\n",
    "    \n",
    "    if not os.path.exists(osm_file_path):\n",
    "        print(f\"Warning: OSM file for year {year} not found: {osm_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing educational institute data for year {year} using {osm_file_path}...\")\n",
    "\n",
    "    # Extract educational institution locations\n",
    "    education_df = extract_educational_institutes(osm_file_path)\n",
    "\n",
    "    # Iterate over stations for this year\n",
    "    for index, station in station_df[station_df['year'] == year].iterrows():\n",
    "        lat_s, lon_s, station_name = station['latitude'], station['longitude'], station['station_name']\n",
    "\n",
    "        # Compute distances to all educational institutions\n",
    "        distances = education_df.apply(lambda edu: haversine(lat_s, lon_s, edu['lat'], edu['lon']), axis=1)\n",
    "\n",
    "        # Count institutions within each radius\n",
    "        education_counts = {f'education_within_{r//1000}km': (distances <= r).sum() for r in radii}\n",
    "\n",
    "        # Store results\n",
    "        education_results.append({\n",
    "            'station_name': station_name,\n",
    "            'year': year,\n",
    "            **education_counts\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb96f5d5-690f-4746-9bd7-5757e4b4069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          station_name   latitude  longitude  year  education_within_0km  \\\n",
      "0       Schwedter Steg  52.549072  13.400367  2015                    11   \n",
      "1      Jannowitzbrücke  52.513936  13.417722  2015                     3   \n",
      "2  Prinzregentenstraße  52.488136  13.333120  2015                     9   \n",
      "3          Yorckstraße  52.492110  13.373341  2015                     8   \n",
      "4           Markstraße  52.558190  13.364944  2015                     3   \n",
      "\n",
      "   education_within_1km  education_within_2km  education_within_5km  \n",
      "0                    30                   114                   438  \n",
      "1                    13                   116                   630  \n",
      "2                    59                   191                   618  \n",
      "3                    32                   146                   706  \n",
      "4                    16                    94                   402  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "education_df = pd.DataFrame(education_results)\n",
    "\n",
    "# Merge educational institute count info into the original station dataset\n",
    "station_df = station_df.merge(education_df, on=['station_name', 'year'], how='left')\n",
    "\n",
    "# Save updated dataset\n",
    "station_df.to_csv(\"stations_with_education_counts.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(station_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fe3244-bbd0-4b41-8653-61cb216c85c8",
   "metadata": {},
   "source": [
    "## Number of Hospitals within a radius of 0.5, 1, 2 & 5km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1266de-48dd-496f-b3ed-8704319add6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the folder containing OSM files\n",
    "osm_folder = \"berlin_osm_datasets\"\n",
    "\n",
    "# And, load the station data\n",
    "station_df = pd.read_csv(\"unique_station_year_berlin.csv\")\n",
    "\n",
    "# And, ensure the year column is treated as an integer\n",
    "station_df['year'] = station_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46504641-f19b-4274-8e13-0b3c36938c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each year to its corresponding OSM file\n",
    "year_to_osm = {year: f\"berlin-{str(year)[-2:]}0101.osm.pbf\" for year in range(2015, 2024)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcabff8-ed45-45ab-b1be-92363af29124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search radii in meters\n",
    "radii = [500, 1000, 2000, 5000]  # 0.5km, 1km, 2km, 5km\n",
    "\n",
    "### Haversine function to calculate distances\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance in meters between two lat/lon points.\"\"\"\n",
    "    R = 6371000  # Radius of Earth in meters\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5dc6c7f-a474-4d03-88ab-7bd0cc9f69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a class to extract hospital-related data from OSM\n",
    "class HospitalHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hospitals = []\n",
    "\n",
    "    def node(self, n):\n",
    "        if ('amenity' in n.tags and n.tags['amenity'] in [\n",
    "            'hospital', 'clinic', 'doctor', 'medical_center'\n",
    "        ]) or ('healthcare' in n.tags and n.tags['healthcare'] in [\n",
    "            'hospital', 'clinic', 'doctor', 'health_center', 'pharmacy'\n",
    "        ]):\n",
    "            self.hospitals.append({\n",
    "                'lat': n.location.lat,\n",
    "                'lon': n.location.lon,\n",
    "                'type': n.tags.get('amenity', n.tags.get('healthcare', 'unknown'))  # Store the type of hospital\n",
    "            })\n",
    "\n",
    "def extract_hospitals(osm_file):\n",
    "    \"\"\"Extract hospital locations from a given OSM file.\"\"\"\n",
    "    handler = HospitalHandler()\n",
    "    \n",
    "    # Enable location lookup\n",
    "    handler.apply_file(osm_file, locations=True)\n",
    "\n",
    "    return pd.DataFrame(handler.hospitals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3025cf-7b23-487d-bf65-114629f44676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hospital data for year 2015 using berlin_osm_datasets\\berlin-150101.osm.pbf...\n",
      "Processing hospital data for year 2016 using berlin_osm_datasets\\berlin-160101.osm.pbf...\n",
      "Processing hospital data for year 2017 using berlin_osm_datasets\\berlin-170101.osm.pbf...\n",
      "Processing hospital data for year 2018 using berlin_osm_datasets\\berlin-180101.osm.pbf...\n",
      "Processing hospital data for year 2019 using berlin_osm_datasets\\berlin-190101.osm.pbf...\n",
      "Processing hospital data for year 2020 using berlin_osm_datasets\\berlin-200101.osm.pbf...\n",
      "Processing hospital data for year 2021 using berlin_osm_datasets\\berlin-210101.osm.pbf...\n",
      "Processing hospital data for year 2022 using berlin_osm_datasets\\berlin-220101.osm.pbf...\n",
      "Processing hospital data for year 2023 using berlin_osm_datasets\\berlin-230101.osm.pbf...\n"
     ]
    }
   ],
   "source": [
    "# List to store hospital count results\n",
    "hospital_results = []\n",
    "\n",
    "# Iterate over unique years\n",
    "for year in station_df['year'].unique():\n",
    "    osm_file_path = os.path.join(osm_folder, year_to_osm[year])\n",
    "    \n",
    "    if not os.path.exists(osm_file_path):\n",
    "        print(f\"Warning: OSM file for year {year} not found: {osm_file_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing hospital data for year {year} using {osm_file_path}...\")\n",
    "\n",
    "    # Extract hospital locations\n",
    "    hospital_df = extract_hospitals(osm_file_path)\n",
    "\n",
    "    # Iterate over stations for this year\n",
    "    for index, station in station_df[station_df['year'] == year].iterrows():\n",
    "        lat_s, lon_s, station_name = station['latitude'], station['longitude'], station['station_name']\n",
    "\n",
    "        # Compute distances to all hospitals\n",
    "        distances = hospital_df.apply(lambda hosp: haversine(lat_s, lon_s, hosp['lat'], hosp['lon']), axis=1)\n",
    "\n",
    "        # Count hospitals within each radius\n",
    "        hospital_counts = {f'hospitals_within_{r//1000}km': (distances <= r).sum() for r in radii}\n",
    "\n",
    "        # Store results\n",
    "        hospital_results.append({\n",
    "            'station_name': station_name,\n",
    "            'year': year,\n",
    "            **hospital_counts\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995bde55-b4a8-4c29-bf42-33fdcf8ff072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          station_name   latitude  longitude  year  hospitals_within_0km  \\\n",
      "0       Schwedter Steg  52.549072  13.400367  2015                     0   \n",
      "1      Jannowitzbrücke  52.513936  13.417722  2015                     0   \n",
      "2  Prinzregentenstraße  52.488136  13.333120  2015                     0   \n",
      "3          Yorckstraße  52.492110  13.373341  2015                     1   \n",
      "4           Markstraße  52.558190  13.364944  2015                     0   \n",
      "\n",
      "   hospitals_within_1km  hospitals_within_2km  hospitals_within_5km  \n",
      "0                     0                     0                     4  \n",
      "1                     0                     0                     6  \n",
      "2                     0                     2                     6  \n",
      "3                     1                     3                     7  \n",
      "4                     0                     1                     3  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "hospital_df = pd.DataFrame(hospital_results)\n",
    "\n",
    "# Merge hospital count info into the original station dataset\n",
    "station_df = station_df.merge(hospital_df, on=['station_name', 'year'], how='left')\n",
    "\n",
    "# Save updated dataset\n",
    "station_df.to_csv(\"stations_with_hospitals.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(station_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee04ef-bf3d-439b-bf02-617d3f92c375",
   "metadata": {},
   "source": [
    "## Merge the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6220dc-cb1e-4464-8328-82a90b5b417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now first let's merge all the infrastructure files\n",
    "# Let's load each of the files\n",
    "df_speed = pd.read_csv('stations_with_maxspeed.csv')\n",
    "df_lane_type = pd.read_csv('stations_with_bicycle_lane_type.csv')\n",
    "df_shops = pd.read_csv('stations_with_shops.csv')\n",
    "df_hotels = pd.read_csv('stations_with_hotels.csv')\n",
    "df_edu = pd.read_csv('stations_with_education_counts.csv')\n",
    "df_hospitals = pd.read_csv('stations_with_hospitals.csv')\n",
    "\n",
    "# Now, let's define common merge keys\n",
    "merge_keys = ['station_name', 'latitude', 'longitude', 'year']\n",
    "\n",
    "# And, merge all dataframes one by one on common keys\n",
    "df_merged = df_speed.merge(df_lane_type, on=merge_keys, how='outer') \\\n",
    "                    .merge(df_shops, on=merge_keys, how='outer') \\\n",
    "                    .merge(df_hotels, on=merge_keys, how='outer') \\\n",
    "                    .merge(df_edu, on=merge_keys, how='outer') \\\n",
    "                    .merge(df_hospitals, on=merge_keys, how='outer')\n",
    "\n",
    "# And, finally save to a new CSV file\n",
    "df_merged.to_csv('infrastructure_data_berlin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c747d6c-5870-451b-9465-58f903eec189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   station_name           150 non-null    object \n",
      " 1   latitude               150 non-null    float64\n",
      " 2   longitude              150 non-null    float64\n",
      " 3   year                   150 non-null    int64  \n",
      " 4   maxspeed_near_station  146 non-null    float64\n",
      " 5   bicycle_lane_type      150 non-null    object \n",
      " 6   shops_within_0km       150 non-null    int64  \n",
      " 7   shops_within_1km       150 non-null    int64  \n",
      " 8   shops_within_2km       150 non-null    int64  \n",
      " 9   shops_within_5km       150 non-null    int64  \n",
      " 10  hotels_within_0km      150 non-null    int64  \n",
      " 11  hotels_within_1km      150 non-null    int64  \n",
      " 12  hotels_within_2km      150 non-null    int64  \n",
      " 13  hotels_within_5km      150 non-null    int64  \n",
      " 14  education_within_0km   150 non-null    int64  \n",
      " 15  education_within_1km   150 non-null    int64  \n",
      " 16  education_within_2km   150 non-null    int64  \n",
      " 17  education_within_5km   150 non-null    int64  \n",
      " 18  hospitals_within_0km   150 non-null    int64  \n",
      " 19  hospitals_within_1km   150 non-null    int64  \n",
      " 20  hospitals_within_2km   150 non-null    int64  \n",
      " 21  hospitals_within_5km   150 non-null    int64  \n",
      "dtypes: float64(3), int64(17), object(2)\n",
      "memory usage: 25.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd7f2ae-f429-4585-8ce3-a794020c28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's merge with the final bicycle dataset\n",
    "# Let's load the infrastructure data\n",
    "infra_df = pd.read_csv('infrastructure_data_berlin.csv')\n",
    "\n",
    "# Now, let's load the cycling data\n",
    "cycling_df = pd.read_csv('cycling_data_berlin_08032025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e2f428-d4e8-45ea-9686-be2a0e62f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the merge process, let's add the year column to the bicycle data\n",
    "cycling_df['timestamp'] = pd.to_datetime(cycling_df['timestamp'])\n",
    "cycling_df['year'] = cycling_df['timestamp'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff67a0e-38ec-4d6e-af0e-a37a0ddf144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, merge on common keys\n",
    "merge_keys = ['station_name', 'latitude', 'longitude', 'year']\n",
    "merged_df = cycling_df.merge(infra_df, on=merge_keys, how='left')\n",
    "\n",
    "# And, save the dataset with infra data\n",
    "merged_df.to_csv('cycle_infra_data_berlin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed74443e-c4d6-46c9-b469-3bd6895fa142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1248009 entries, 0 to 1248008\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   timestamp              1248009 non-null  datetime64[ns]\n",
      " 1   station_name           1248009 non-null  object        \n",
      " 2   latitude               1248009 non-null  float64       \n",
      " 3   longitude              1248009 non-null  float64       \n",
      " 4   cycling_volume         1248009 non-null  float64       \n",
      " 5   distance_to_center     1248009 non-null  float64       \n",
      " 6   year                   1248009 non-null  int32         \n",
      " 7   maxspeed_near_station  1216569 non-null  float64       \n",
      " 8   bicycle_lane_type      1248009 non-null  object        \n",
      " 9   shops_within_0km       1248009 non-null  int64         \n",
      " 10  shops_within_1km       1248009 non-null  int64         \n",
      " 11  shops_within_2km       1248009 non-null  int64         \n",
      " 12  shops_within_5km       1248009 non-null  int64         \n",
      " 13  hotels_within_0km      1248009 non-null  int64         \n",
      " 14  hotels_within_1km      1248009 non-null  int64         \n",
      " 15  hotels_within_2km      1248009 non-null  int64         \n",
      " 16  hotels_within_5km      1248009 non-null  int64         \n",
      " 17  education_within_0km   1248009 non-null  int64         \n",
      " 18  education_within_1km   1248009 non-null  int64         \n",
      " 19  education_within_2km   1248009 non-null  int64         \n",
      " 20  education_within_5km   1248009 non-null  int64         \n",
      " 21  hospitals_within_0km   1248009 non-null  int64         \n",
      " 22  hospitals_within_1km   1248009 non-null  int64         \n",
      " 23  hospitals_within_2km   1248009 non-null  int64         \n",
      " 24  hospitals_within_5km   1248009 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int32(1), int64(16), object(2)\n",
      "memory usage: 233.3+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7706db-59c3-4d55-810b-b2e94e7a227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cycling_volume</th>\n",
       "      <th>distance_to_center</th>\n",
       "      <th>year</th>\n",
       "      <th>maxspeed_near_station</th>\n",
       "      <th>bicycle_lane_type</th>\n",
       "      <th>shops_within_0km</th>\n",
       "      <th>...</th>\n",
       "      <th>hotels_within_2km</th>\n",
       "      <th>hotels_within_5km</th>\n",
       "      <th>education_within_0km</th>\n",
       "      <th>education_within_1km</th>\n",
       "      <th>education_within_2km</th>\n",
       "      <th>education_within_5km</th>\n",
       "      <th>hospitals_within_0km</th>\n",
       "      <th>hospitals_within_1km</th>\n",
       "      <th>hospitals_within_2km</th>\n",
       "      <th>hospitals_within_5km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.137726</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.0</td>\n",
       "      <td>none</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>114</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.137726</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.0</td>\n",
       "      <td>none</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>114</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.137726</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.0</td>\n",
       "      <td>none</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>114</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.137726</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.0</td>\n",
       "      <td>none</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>114</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>Schwedter Steg</td>\n",
       "      <td>52.549072</td>\n",
       "      <td>13.400367</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.137726</td>\n",
       "      <td>2015</td>\n",
       "      <td>70.0</td>\n",
       "      <td>none</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>115</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>114</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp    station_name   latitude  longitude  cycling_volume  \\\n",
       "0 2015-01-01 00:00:00  Schwedter Steg  52.549072  13.400367             8.0   \n",
       "1 2015-01-01 01:00:00  Schwedter Steg  52.549072  13.400367            10.0   \n",
       "2 2015-01-01 02:00:00  Schwedter Steg  52.549072  13.400367             8.0   \n",
       "3 2015-01-01 03:00:00  Schwedter Steg  52.549072  13.400367             6.0   \n",
       "4 2015-01-01 04:00:00  Schwedter Steg  52.549072  13.400367             6.0   \n",
       "\n",
       "   distance_to_center  year  maxspeed_near_station bicycle_lane_type  \\\n",
       "0            3.137726  2015                   70.0              none   \n",
       "1            3.137726  2015                   70.0              none   \n",
       "2            3.137726  2015                   70.0              none   \n",
       "3            3.137726  2015                   70.0              none   \n",
       "4            3.137726  2015                   70.0              none   \n",
       "\n",
       "   shops_within_0km  ...  hotels_within_2km  hotels_within_5km  \\\n",
       "0                 6  ...                 11                115   \n",
       "1                 6  ...                 11                115   \n",
       "2                 6  ...                 11                115   \n",
       "3                 6  ...                 11                115   \n",
       "4                 6  ...                 11                115   \n",
       "\n",
       "   education_within_0km  education_within_1km  education_within_2km  \\\n",
       "0                    11                    30                   114   \n",
       "1                    11                    30                   114   \n",
       "2                    11                    30                   114   \n",
       "3                    11                    30                   114   \n",
       "4                    11                    30                   114   \n",
       "\n",
       "   education_within_5km  hospitals_within_0km  hospitals_within_1km  \\\n",
       "0                   438                     0                     0   \n",
       "1                   438                     0                     0   \n",
       "2                   438                     0                     0   \n",
       "3                   438                     0                     0   \n",
       "4                   438                     0                     0   \n",
       "\n",
       "   hospitals_within_2km  hospitals_within_5km  \n",
       "0                     0                     4  \n",
       "1                     0                     4  \n",
       "2                     0                     4  \n",
       "3                     0                     4  \n",
       "4                     0                     4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff4ea9-6c49-49a4-a1f1-6f19a286f543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
