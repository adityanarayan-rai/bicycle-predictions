{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup using the LOGO-CV Approach - With Feature Selection and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV, SequentialFeatureSelector, SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data\n",
    "df = pd.read_csv(\"processed_data_newyork_15042025.csv\")\n",
    "\n",
    "# Filter stations with > 2 years of data\n",
    "station_years = df.groupby('name')['year'].nunique()\n",
    "valid_stations = station_years[station_years > 2].index\n",
    "df = df[df['name'].isin(valid_stations)]\n",
    "\n",
    "# Let's do the log transformation of the counts variable\n",
    "df['log_counts'] = np.log1p(df['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Let's set up LOGO-CV\n",
    "logo = LeaveOneGroupOut()\n",
    "groups = df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all predictions\n",
    "all_actuals, all_predictions = [], []\n",
    "aadb_actual_list, aadb_pred_list, aadb_station_list = [], [], []\n",
    "\n",
    "# Run LOGO-CV\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    \n",
    "    # Compute mean log volume per station (baseline)\n",
    "    mean_log_volume = train_df['log_counts'].mean()\n",
    "    pred_log = np.full(len(test_df), mean_log_volume)\n",
    "    pred = np.expm1(pred_log)\n",
    "\n",
    "    y_true = test_df['counts'].values\n",
    "\n",
    "    # Daily predictions\n",
    "    all_actuals.extend(y_true)\n",
    "    all_predictions.extend(pred)\n",
    "\n",
    "    # AADB-level for this test station\n",
    "    aadb_actual = y_true.mean()\n",
    "    aadb_pred = pred.mean()\n",
    "    aadb_actual_list.append(aadb_actual)\n",
    "    aadb_pred_list.append(aadb_pred)\n",
    "    aadb_station_list.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to arrays\n",
    "y_true_all = np.array(all_actuals)\n",
    "y_pred_all = np.array(all_predictions)\n",
    "\n",
    "# Daily metrics\n",
    "mae_daily = mean_absolute_error(y_true_all, y_pred_all)\n",
    "rmse_daily = np.sqrt(mean_squared_error(y_true_all, y_pred_all))\n",
    "r2_daily = r2_score(y_true_all, y_pred_all)\n",
    "smape_daily = smape(y_true_all, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB-level metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_station_list,\n",
    "    'actual': aadb_actual_list,\n",
    "    'predicted': aadb_pred_list\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Baseline Model Evaluation:\n",
      "Daily  → MAE: 1792.31, RMSE: 2455.68, R²: -0.40, SMAPE: 99.59%\n",
      "AADB   → MAE: 1521.75, RMSE: 1887.84, R²: -0.67, SMAPE: 89.45%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nLOGO Baseline Model Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by RFE for Linear Regression:\n",
      "['Tree Canopy']\n"
     ]
    }
   ],
   "source": [
    "# RFE with Linear Regression\n",
    "lr = LinearRegression()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('feature_selection', RFECV(\n",
    "        estimator=lr,\n",
    "        step=1,\n",
    "        cv=KFold(n_splits=5),\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('regression', lr)\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Extract selected features\n",
    "selected_features = X.columns[pipeline.named_steps['feature_selection'].support_]\n",
    "print(\"\\nSelected features by RFE for Linear Regression:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LOGO grouping\n",
    "groups = df['name']\n",
    "\n",
    "# Prep for storing results\n",
    "all_true, all_preds = [], []\n",
    "aadb_actuals, aadb_preds, aadb_stations = [], [], []\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    # Scale inputs\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train and predict\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_log = model.predict(X_test_scaled)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Evaluation\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Evaluation\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Linear Regression Evaluation:\n",
      "Daily  → MAE: 1858.77, RMSE: 2498.49, R²: -0.45, SMAPE: 107.17%\n",
      "AADB   → MAE: 1588.32, RMSE: 1934.88, R²: -0.75, SMAPE: 97.33%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Linear Regression Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by SFS for Decision Tree:\n",
      "['shops_within_0km', 'hotels_within_0km', 'education_within_0km', 'tavg', 'tmax', 'prcp', 'snow', 'wspd', 'pres', 'public_holiday', 'city_avg_motor_volume', 'day_of_week', 'is_weekend', 'month', 'bicycle_lane_type_lane', 'bicycle_lane_type_none', 'bicycle_lane_type_track', 'Grass/Shrubs', 'Roads', 'Other Impervious', 'Buildings', 'Railroads', 'Bare Soil', 'Water']\n"
     ]
    }
   ],
   "source": [
    "# Base model for feature selection\n",
    "base_tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Sequential forward selection\n",
    "sfs = SequentialFeatureSelector(\n",
    "    base_tree,\n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "print(\"\\nSelected features by SFS for Decision Tree:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "\n",
      "Best hyperparameters:\n",
      "{'criterion': 'squared_error', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features from SFS\n",
    "X_selected = df[selected_features]\n",
    "y = df['log_counts']\n",
    "\n",
    "# Parameter grid for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    tree,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Save best model for reuse\n",
    "best_tree_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Setup LOGO\n",
    "groups = df['name']\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over stations\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    # Train with best hyperparameters\n",
    "    tree_model = DecisionTreeRegressor(**grid_search.best_params_, random_state=42)\n",
    "    tree_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_log = tree_model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Evaluation\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Evaluation\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Decision Tree Evaluation:\n",
      "Daily  → MAE: 1737.04, RMSE: 2247.00, R²: -0.17, SMAPE: 95.60%\n",
      "AADB   → MAE: 1490.42, RMSE: 1772.57, R²: -0.47, SMAPE: 87.05%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nLOGO Decision Tree Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "\n",
      "Selected features by RFE for Random Forest:\n",
      "['longitude']\n"
     ]
    }
   ],
   "source": [
    "# RFECV with Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=rf_model,\n",
    "    step=1,\n",
    "    cv=KFold(n_splits=5),\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X.columns[rfecv.support_]\n",
    "print(\"\\nSelected features by RFE for Random Forest:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Best hyperparameters for Random Forest:\n",
      "{'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_selected = df[selected_features]\n",
    "y = df['log_counts']\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Random Forest:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store best model\n",
    "best_rf_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = df['name']\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGO loop\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    model = RandomForestRegressor(**grid_search.best_params_, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE function\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Random Forest Evaluation:\n",
      "Daily  → MAE: 1735.17, RMSE: 2300.04, R²: -0.23, SMAPE: 86.12%\n",
      "AADB   → MAE: 1665.40, RMSE: 2003.50, R²: -0.88, SMAPE: 85.42%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Random Forest Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 47 features.\n",
      "Fitting estimator with 46 features.\n",
      "Fitting estimator with 45 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 43 features.\n",
      "Fitting estimator with 42 features.\n",
      "Fitting estimator with 41 features.\n",
      "Fitting estimator with 40 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 38 features.\n",
      "Fitting estimator with 37 features.\n",
      "Fitting estimator with 36 features.\n",
      "Fitting estimator with 35 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 33 features.\n",
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 31 features.\n",
      "Fitting estimator with 30 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 28 features.\n",
      "Fitting estimator with 27 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 25 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "\n",
      "Selected features by RFE for Gradient Boosting:\n",
      "['longitude']\n"
     ]
    }
   ],
   "source": [
    "# RFECV with Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=gb_model,\n",
    "    step=1,\n",
    "    cv=KFold(n_splits=5),\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "selected_features = X.columns[rfecv.support_]\n",
    "print(\"\\nSelected features by RFE for Gradient Boosting:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "Best hyperparameters for Gradient Boosting:\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_selected = df[selected_features]\n",
    "y = df['log_counts']\n",
    "\n",
    "# Parameter grid for Gradient Boosting\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Gradient Boosting:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Save best model\n",
    "best_gb_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = df['name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    gb_model = GradientBoostingRegressor(**grid_search.best_params_, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    y_pred_log = gb_model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Gradient Boosting Evaluation:\n",
      "Daily  → MAE: 1697.69, RMSE: 2271.83, R²: -0.20, SMAPE: 85.55%\n",
      "AADB   → MAE: 1621.63, RMSE: 1968.30, R²: -0.82, SMAPE: 84.37%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Gradient Boosting Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by SFS for XGBoost:\n",
      "['hotels_within_1km', 'education_within_0km', 'education_within_1km', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wspd', 'pres', 'wdir', 'public_holiday', 'total_motor_volume', 'city_avg_motor_volume', 'city_total_motor_volume', 'day_of_week', 'is_weekend', 'month', 'bicycle_lane_type_lane', 'bicycle_lane_type_track', 'Grass/Shrubs', 'Roads', 'Bare Soil', 'Water']\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Sequential Forward Selection\n",
    "sfs = SequentialFeatureSelector(\n",
    "    xgb_model,\n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "print(\"\\nSelected features by SFS for XGBoost:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "\n",
      "Best hyperparameters for XGBoost:\n",
      "{'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_selected = df[selected_features]\n",
    "y = df['log_counts']\n",
    "\n",
    "# Parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [5, 10],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_weight': [1, 5],\n",
    "    'gamma': [0, 1]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_selected, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for XGBoost:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store best model\n",
    "best_xgb_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = df['name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    xgb_model = XGBRegressor(**grid_search.best_params_, objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_log = xgb_model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO XGBoost Evaluation:\n",
      "Daily  → MAE: 1977.07, RMSE: 2844.84, R²: -0.88, SMAPE: 86.95%\n",
      "AADB   → MAE: 1480.54, RMSE: 2151.80, R²: -1.17, SMAPE: 66.27%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO XGBoost Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by SelectKBest for SVR:\n",
      "['latitude', 'longitude', 'distance_to_center_km', 'shops_within_2km', 'hotels_within_5km', 'tavg', 'tmin', 'tmax', 'avg_motor_volume', 'Railroads']\n"
     ]
    }
   ],
   "source": [
    "# Use scaling + SelectKBest\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select_k_best', SelectKBest(score_func=f_regression, k=10))\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Get selected feature names\n",
    "mask = pipeline.named_steps['select_k_best'].get_support()\n",
    "selected_features = X.columns[mask]\n",
    "\n",
    "print(\"\\nSelected features by SelectKBest for SVR:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "\n",
      "Best hyperparameters for SVR:\n",
      "{'C': 1, 'degree': 2, 'epsilon': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_selected = df[selected_features]\n",
    "y = df['log_counts']\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Parameter grid for SVR\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [1, 10, 100],\n",
    "    'epsilon': [0.1, 0.5],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3]  # used only for 'poly'\n",
    "}\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    svr,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for SVR:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Save best config\n",
    "best_svr_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define groups\n",
    "groups = df['name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    svr_model = SVR(**best_svr_params)\n",
    "    svr_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_log = svr_model.predict(X_test_scaled)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO SVR Evaluation:\n",
      "Daily  → MAE: 130599.81, RMSE: 1114164.27, R²: -288361.35, SMAPE: 135.70%\n",
      "AADB   → MAE: 116671.79, RMSE: 403750.71, R²: -76400.89, SMAPE: 129.62%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO SVR Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features by SelectKBest for Shallow Neural Network:\n",
      "['latitude', 'longitude', 'distance_to_center_km', 'shops_within_0km', 'shops_within_1km', 'shops_within_2km', 'hotels_within_2km', 'hotels_within_5km', 'education_within_1km', 'education_within_5km', 'hospitals_within_0km', 'hospitals_within_1km', 'hospitals_within_2km', 'tavg', 'tmin', 'tmax', 'avg_motor_volume', 'total_motor_volume', 'bicycle_lane_type_none', 'bicycle_lane_type_unknown', 'Tree Canopy', 'Other Impervious', 'Railroads', 'Water']\n"
     ]
    }
   ],
   "source": [
    "# Pipeline: scale → SelectKBest\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select_k_best', SelectKBest(score_func=f_regression, k=24))\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Get selected features\n",
    "mask = pipeline.named_steps['select_k_best'].get_support()\n",
    "selected_features = X.columns[mask]\n",
    "\n",
    "print(\"\\nSelected features by SelectKBest for Shallow Neural Network:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Best hyperparameters for Shallow Neural Network:\n",
      "{'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate_init': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Use the selected features\n",
    "X_selected = df[selected_features]\n",
    "y = df['log_counts']\n",
    "\n",
    "# Scale inputs\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Parameter grid for SNN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor(max_iter=500, random_state=42)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    mlp,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Shallow Neural Network:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store config\n",
    "best_snn_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Groups\n",
    "groups = df['name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    mlp_model = MLPRegressor(**best_snn_params, max_iter=500, random_state=42)\n",
    "    mlp_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_log = mlp_model.predict(X_test_scaled)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Shallow Neural Network Evaluation:\n",
      "Daily  → MAE: 29321.01, RMSE: 162952.12, R²: -6167.22, SMAPE: 130.69%\n",
      "AADB   → MAE: 23258.48, RMSE: 56956.26, R²: -1519.41, SMAPE: 126.35%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Shallow Neural Network Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "X = df[features]\n",
    "y = df['log_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Best hyperparameters for Deep Neural Network:\n",
      "{'mlp__activation': 'relu', 'mlp__hidden_layer_sizes': (100, 50), 'mlp__learning_rate_init': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline for scaling + PCA + MLP\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=20)),\n",
    "    ('mlp', MLPRegressor(max_iter=500, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid for SNN\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(50, 50), (100, 50), (100, 100)],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=KFold(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"\\nBest hyperparameters for Deep Neural Network:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Store best model\n",
    "best_dnn_pipeline = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Groups\n",
    "groups = df['name']\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage\n",
    "all_preds, all_true = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "# LOGO Loop\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df['log_counts']\n",
    "    X_test = test_df[features]\n",
    "    y_test_true = test_df['counts']\n",
    "\n",
    "    # Rebuild same pipeline with best params\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=20)),\n",
    "        ('mlp', MLPRegressor(\n",
    "            hidden_layer_sizes=grid_search.best_params_['mlp__hidden_layer_sizes'],\n",
    "            activation=grid_search.best_params_['mlp__activation'],\n",
    "            learning_rate_init=grid_search.best_params_['mlp__learning_rate_init'],\n",
    "            max_iter=500,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_log = pipeline.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_true.extend(y_test_true.values)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true.values))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )\n",
    "\n",
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_true, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "r2_daily = r2_score(all_true, all_preds)\n",
    "smape_daily = smape(np.array(all_true), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO Deep Neural Network Evaluation:\n",
      "Daily  → MAE: 60955.25, RMSE: 210418.12, R²: -10284.05, SMAPE: 110.86%\n",
      "AADB   → MAE: 43398.84, RMSE: 151671.47, R²: -10780.65, SMAPE: 98.34%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO Deep Neural Network Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "n_steps = 30\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "lstm_units = 64\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "# Define features and target\n",
    "non_features = ['name', 'date', 'year', 'counts', 'log_counts']\n",
    "features = [col for col in df.columns if col not in non_features]\n",
    "\n",
    "groups = df['name']\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 / len(y_true) * np.sum(\n",
    "        2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 3ms/step\n",
      "52/52 [==============================] - 0s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "114/114 [==============================] - 1s 2ms/step\n",
      "57/57 [==============================] - 0s 2ms/step\n",
      "113/113 [==============================] - 1s 2ms/step\n",
      "91/91 [==============================] - 1s 2ms/step\n",
      "114/114 [==============================] - 1s 2ms/step\n",
      "91/91 [==============================] - 1s 2ms/step\n",
      "79/79 [==============================] - 1s 3ms/step\n",
      "99/99 [==============================] - 1s 2ms/step\n",
      "113/113 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Storage\n",
    "all_preds, all_actuals = [], []\n",
    "aadb_preds, aadb_actuals, aadb_stations = [], [], []\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train_idx, test_idx in logo.split(df, groups=groups):\n",
    "    train_df = df.iloc[train_idx].sort_values('date')\n",
    "    test_df = df.iloc[test_idx].sort_values('date')\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(train_df[features])\n",
    "    X_test_scaled = scaler.transform(test_df[features])\n",
    "\n",
    "    y_train = train_df['log_counts'].values\n",
    "    y_test_true = test_df['counts'].values\n",
    "    y_test_log = test_df['log_counts'].values\n",
    "\n",
    "    # Build sequences\n",
    "    def create_sequences(X, y, n_steps):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(n_steps, len(X)):\n",
    "            X_seq.append(X[i - n_steps:i])\n",
    "            y_seq.append(y[i])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, n_steps)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_log, n_steps)\n",
    "    y_test_true_seq = y_test_true[n_steps:]\n",
    "\n",
    "    if len(X_test_seq) == 0:\n",
    "        continue\n",
    "\n",
    "    # Build model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, input_shape=(n_steps, len(features))))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "    model.fit(X_train_seq, y_train_seq, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_log = model.predict(X_test_seq).flatten()\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    all_preds.extend(y_pred)\n",
    "    all_actuals.extend(y_test_true_seq)\n",
    "\n",
    "    aadb_preds.append(np.mean(y_pred))\n",
    "    aadb_actuals.append(np.mean(y_test_true_seq))\n",
    "    aadb_stations.append(test_df['name'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Metrics\n",
    "mae_daily = mean_absolute_error(all_actuals, all_preds)\n",
    "rmse_daily = np.sqrt(mean_squared_error(all_actuals, all_preds))\n",
    "r2_daily = r2_score(all_actuals, all_preds)\n",
    "smape_daily = smape(np.array(all_actuals), np.array(all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADB Metrics\n",
    "aadb_df = pd.DataFrame({\n",
    "    'station': aadb_stations,\n",
    "    'actual': aadb_actuals,\n",
    "    'predicted': aadb_preds\n",
    "})\n",
    "\n",
    "mae_aadb = mean_absolute_error(aadb_df['actual'], aadb_df['predicted'])\n",
    "rmse_aadb = np.sqrt(mean_squared_error(aadb_df['actual'], aadb_df['predicted']))\n",
    "r2_aadb = r2_score(aadb_df['actual'], aadb_df['predicted'])\n",
    "smape_aadb = smape(aadb_df['actual'], aadb_df['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOGO LSTM Evaluation:\n",
      "Daily  → MAE: 2160.96, RMSE: 2982.64, R²: -1.06, SMAPE: 167.87%\n",
      "AADB   → MAE: 1893.68, RMSE: 2407.95, R²: -1.67, SMAPE: 151.63%\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(\"\\nLOGO LSTM Evaluation:\")\n",
    "print(f\"Daily  → MAE: {mae_daily:.2f}, RMSE: {rmse_daily:.2f}, R²: {r2_daily:.2f}, SMAPE: {smape_daily:.2f}%\")\n",
    "print(f\"AADB   → MAE: {mae_aadb:.2f}, RMSE: {rmse_aadb:.2f}, R²: {r2_aadb:.2f}, SMAPE: {smape_aadb:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
